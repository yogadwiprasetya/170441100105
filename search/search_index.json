{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"DATA MINING Pengertian Data Mining : Konsep dan Fungsinya \u2013 Ada banyak sekali sebutan untuk data mining. Mulai dari ekstrasi pengetahuan, analisis data/pola, kecerdasan bisnis dan masih banyak yang lainya. Jika anda kuliah atau belajar tentang pemrograman bisa dipastikan data mining akan menjadi salah satu mata kuliah wajib. Dari pengalaman yang saya alami waktu kulliah dulu ada banyak hal yang harus anda ketahui tentang data mining. Mulai dari pengertian data mining menurut para ahli, tujuan dan manfaat data mining, fungsi dan beserta contohnya. Data mining dapat ditunjang dengan kekayaan dan keanekaragaman dari berbagai bidang ilmu seperti artificial intelligence, database, statistic, permodelan matematika dan pengolahan citra dan lainnya. Kali ini saya akan berbagi pengalaman waktu belajar data mining saat kuliah dulu,. Dari pengertian data mining, konsep, fungsi umum dan lainya akan saya uraikan. Supaya anda bisa lebih mengenal pengertian dan konsep data mining lebih lengkap. Data mining merupakan suatu proses penjelajahan atau mencari otomatis untuk mendapatkan informasi berguna dalam suatu repository data yang sangat besar. Ada banyak nama lain dari data mining seperti Knowledge discovery databases (KDD), knowledge extraction, data/pattern analysis, data archeology, data dredging, information harvesting, business intelligence. Data Mining merupakan bagian yang terintegrasi dari Knowledge Discovery in Databases (KDD). Bertujuan untuk proses transformasi data mentah menjadi informasi berguna. Jika di gambarkan secara detail tahapan KDD menjadi 5 tahap. Berikut Tahapannya : 1. Seleksi Bertujuan mentransformasikan data mentah ke format yang sesuai untuk analisis. Terdiri atas proses seleksi fitur, reduksi dimensionalitas, normalisasi dan subsetting data. Proses penyeleksian atau segmentasi data menurut beberapa criteria. Misal, Orang \u2013 orang yang mempunyai mobil. 2. Preprocessing Bertujuan untuk menjamin bahwa hasil proses data mining yang diintegrasikan pada system penunjang keputusan, benar-benar hasil yang valid. Proses pembersihan data, dimana informasi yang tidak dibutuhkan dibuang. Misal, Jenis kelamin pasien untuk analisis kehamilan. Data dikonfigurasi ulang untuk memastikan format yang konsisten karena berasal dari berbagai sumber. Misal, Jenis kelamin disimpan dengan bentuk f atau m dan 1 atau 0. 3. Transformasi Proses transformasi sehingga data menjadi berguna dan dapat ditelusuri. 4. Data Mining Proses yg berfokus pada ekstraksi pola-pola data. Pola dapat didefinisikan sebagai sekumpulan fakta-fakta (data) F, bahasa L, dan beberapa measure of certainty (pengukuran kepastian) C. Suatu pola dinyatakan S dalam L menggambarkan keterhubungan antara subset Fs dari F dengan kepastian c dimana S adalah simpel dibandingkan perhitungan semua fakta dalam Fs. 5. Interpretasi Evaluasi Pola diidentifikasi sistem, lalu diinterpretasikan sebagai pengetahuan yg dapat digunakan untuk mendukung pengambilan keputusan manusia, contoh : Tugas, prediksi klasifikasi. Meringkas konten suatu database. Menjelaskan fenomena yang diamati. 3. Fungsi Umum Data Mining Fungsi dalam data mining bisa diartikan sebagai sub kegiatan yang ada dalam data mining rangka menemukan, menggali, atau menambang pengetahuan. Secara global fungsi utama data mining adalah sebagai berikut . klasifikasi Proses untuk menyatakan suatu objek kesalah satu kategori yang sudah didefinisikan sebelumnya. Proses pembelajaran fungsi target (model klasifikasi) yang memetakan setiap sekumpulan atribut x (input) kesalah satu kelas yang didefinisikan sebelumnya. Input : sekumpulan record (training set). Setiap record terdiri atas sekumpulan atribut, salah satu atribut adalah kelas. Mencari model untuk atribut kelas sebagai fungsi dari nilai-nilai untuk atribut yang lain. Tujuannya adalah record-record yang sebelumnya tidak terlihat dinyatakan kelasnya seakurat mungkin. Ada banyak sekali algoritma yang terdapat pada teknik klasifikasi , berberapa diantaranya seperti : Holte Prism Na\u00efve Bayes ID3 K-Nearest Neighbor Pengelompokan / Clustering Clustering adalah metode penganalisaan data yang tujuannya adalah untuk mengelompokkan data dengan karakteristik yang sama ke suatu \u2018wilayah\u2019 yang sama dan data dengan karakteristik yang berbeda ke \u2018wilayah\u2019 yang lain. Perbedaan clustering dengan classification Dataset yang digunakan pada clustering tidak menampilkan class / target attribute , sedangkan dataset yang digunakan pada classification mutlak harus menampilkan class / target attribute . Pengetahuan yang dihasilkan oleh metode clustering berupa cluster hasil pengelompokan, sedangkan pengetahuan yang dihasilkan oleh metode classification berupa selain cluster (bisa Decision Tree, Ruleset, Weight pada BackPropagation , dan lain-lain). clustering dipakai ketika tidak diketahuinya bagaimana data harus dikelompokkan. Jumlah kelompok diasumsikan sendiri tanpa ditentukan terlebih dahulu. Keluaran pendekatan ini adalah data yang sudah dikelompokkan. Sedangkan classification, terdapat informasi mengenai bagaimana data tersebut dikelompokkan. Kemudian dilakukan training pada sistem dengan data yang sudah diberikan label (ke dalam kelompok manakah data tersebut dikelompokkan), selanjutnya sistem akan mengklasifikasikan data-data yang baru ke dalam kelompok yang ada. Tidak akan ada pertambahan kelompok. Algoritma yang sering di pakai untuk pengelompokan data adalah Algoritma K-Means. Asosiasi Analisis asosiasi atau association rule mining adalah teknik data mining untuk menemukan aturan assosiatif antara suatu kombinasi item. Contoh aturan assosiatif dari analisa pembelian di suatu pasar swalayan adalah dapat diketahuinya berapa besar kemungkinan seorang pelanggan membeli roti bersamaan dengan susu. Dengan pengetahuan tersebut pemilik pasar swalayan dapat mengatur penempatan barangnya ataumerancang kampanye pemasaran dengan memakai kupon diskon untuk kombinasi barang tertentu. Analisis asosiasi menjadi terkenal karena aplikasinya untuk menganalisa isi keranjang belanja di pasar swalayan. Analisis asosiasi juga sering disebut dengan istilah market basket analysis. Analisis asosiasi dikenal juga sebagai salah satu teknik data mining yang menjadi dasar dari berbagai teknik data mining lainnya. Khususnya salah satu tahap dari analisis asosiasi yang disebut analisis pola frequensi tinggi (frequent pattern mining) menarik perhatian banyak peneliti untuk menghasilkan algoritma yang efisien. Penting tidaknya suatu aturan assosiatif dapat diketahui dengan dua parameter, support (nilai penunjang) yaitu persentase kombinasi item tersebut dalam database dan confidence (nilai kepastian) yaitu kuatnya hubungan antar item dalam aturan assosiatif. referensi http://kabarbee.com/pengertian-data-mining/","title":"Pengantar"},{"location":"#data-mining","text":"Pengertian Data Mining : Konsep dan Fungsinya \u2013 Ada banyak sekali sebutan untuk data mining. Mulai dari ekstrasi pengetahuan, analisis data/pola, kecerdasan bisnis dan masih banyak yang lainya. Jika anda kuliah atau belajar tentang pemrograman bisa dipastikan data mining akan menjadi salah satu mata kuliah wajib. Dari pengalaman yang saya alami waktu kulliah dulu ada banyak hal yang harus anda ketahui tentang data mining. Mulai dari pengertian data mining menurut para ahli, tujuan dan manfaat data mining, fungsi dan beserta contohnya. Data mining dapat ditunjang dengan kekayaan dan keanekaragaman dari berbagai bidang ilmu seperti artificial intelligence, database, statistic, permodelan matematika dan pengolahan citra dan lainnya. Kali ini saya akan berbagi pengalaman waktu belajar data mining saat kuliah dulu,. Dari pengertian data mining, konsep, fungsi umum dan lainya akan saya uraikan. Supaya anda bisa lebih mengenal pengertian dan konsep data mining lebih lengkap. Data mining merupakan suatu proses penjelajahan atau mencari otomatis untuk mendapatkan informasi berguna dalam suatu repository data yang sangat besar. Ada banyak nama lain dari data mining seperti Knowledge discovery databases (KDD), knowledge extraction, data/pattern analysis, data archeology, data dredging, information harvesting, business intelligence. Data Mining merupakan bagian yang terintegrasi dari Knowledge Discovery in Databases (KDD). Bertujuan untuk proses transformasi data mentah menjadi informasi berguna. Jika di gambarkan secara detail tahapan KDD menjadi 5 tahap. Berikut Tahapannya :","title":"DATA MINING"},{"location":"#1-seleksi","text":"Bertujuan mentransformasikan data mentah ke format yang sesuai untuk analisis. Terdiri atas proses seleksi fitur, reduksi dimensionalitas, normalisasi dan subsetting data. Proses penyeleksian atau segmentasi data menurut beberapa criteria. Misal, Orang \u2013 orang yang mempunyai mobil.","title":"1. Seleksi"},{"location":"#2-preprocessing","text":"Bertujuan untuk menjamin bahwa hasil proses data mining yang diintegrasikan pada system penunjang keputusan, benar-benar hasil yang valid. Proses pembersihan data, dimana informasi yang tidak dibutuhkan dibuang. Misal, Jenis kelamin pasien untuk analisis kehamilan. Data dikonfigurasi ulang untuk memastikan format yang konsisten karena berasal dari berbagai sumber. Misal, Jenis kelamin disimpan dengan bentuk f atau m dan 1 atau 0.","title":"2. Preprocessing"},{"location":"#3-transformasi","text":"Proses transformasi sehingga data menjadi berguna dan dapat ditelusuri.","title":"3. Transformasi"},{"location":"#4-data-mining","text":"Proses yg berfokus pada ekstraksi pola-pola data. Pola dapat didefinisikan sebagai sekumpulan fakta-fakta (data) F, bahasa L, dan beberapa measure of certainty (pengukuran kepastian) C. Suatu pola dinyatakan S dalam L menggambarkan keterhubungan antara subset Fs dari F dengan kepastian c dimana S adalah simpel dibandingkan perhitungan semua fakta dalam Fs.","title":"4. Data Mining"},{"location":"#5-interpretasi-evaluasi","text":"Pola diidentifikasi sistem, lalu diinterpretasikan sebagai pengetahuan yg dapat digunakan untuk mendukung pengambilan keputusan manusia, contoh : Tugas, prediksi klasifikasi. Meringkas konten suatu database. Menjelaskan fenomena yang diamati.","title":"5. Interpretasi &amp; Evaluasi"},{"location":"#3-fungsi-umum-data-mining","text":"Fungsi dalam data mining bisa diartikan sebagai sub kegiatan yang ada dalam data mining rangka menemukan, menggali, atau menambang pengetahuan. Secara global fungsi utama data mining adalah sebagai berikut .","title":"3. Fungsi Umum Data Mining"},{"location":"#klasifikasi","text":"Proses untuk menyatakan suatu objek kesalah satu kategori yang sudah didefinisikan sebelumnya. Proses pembelajaran fungsi target (model klasifikasi) yang memetakan setiap sekumpulan atribut x (input) kesalah satu kelas yang didefinisikan sebelumnya. Input : sekumpulan record (training set). Setiap record terdiri atas sekumpulan atribut, salah satu atribut adalah kelas. Mencari model untuk atribut kelas sebagai fungsi dari nilai-nilai untuk atribut yang lain. Tujuannya adalah record-record yang sebelumnya tidak terlihat dinyatakan kelasnya seakurat mungkin. Ada banyak sekali algoritma yang terdapat pada teknik klasifikasi , berberapa diantaranya seperti : Holte Prism Na\u00efve Bayes ID3 K-Nearest Neighbor","title":"klasifikasi"},{"location":"#pengelompokan-clustering","text":"Clustering adalah metode penganalisaan data yang tujuannya adalah untuk mengelompokkan data dengan karakteristik yang sama ke suatu \u2018wilayah\u2019 yang sama dan data dengan karakteristik yang berbeda ke \u2018wilayah\u2019 yang lain. Perbedaan clustering dengan classification Dataset yang digunakan pada clustering tidak menampilkan class / target attribute , sedangkan dataset yang digunakan pada classification mutlak harus menampilkan class / target attribute . Pengetahuan yang dihasilkan oleh metode clustering berupa cluster hasil pengelompokan, sedangkan pengetahuan yang dihasilkan oleh metode classification berupa selain cluster (bisa Decision Tree, Ruleset, Weight pada BackPropagation , dan lain-lain). clustering dipakai ketika tidak diketahuinya bagaimana data harus dikelompokkan. Jumlah kelompok diasumsikan sendiri tanpa ditentukan terlebih dahulu. Keluaran pendekatan ini adalah data yang sudah dikelompokkan. Sedangkan classification, terdapat informasi mengenai bagaimana data tersebut dikelompokkan. Kemudian dilakukan training pada sistem dengan data yang sudah diberikan label (ke dalam kelompok manakah data tersebut dikelompokkan), selanjutnya sistem akan mengklasifikasikan data-data yang baru ke dalam kelompok yang ada. Tidak akan ada pertambahan kelompok. Algoritma yang sering di pakai untuk pengelompokan data adalah Algoritma K-Means.","title":"Pengelompokan / Clustering"},{"location":"#asosiasi","text":"Analisis asosiasi atau association rule mining adalah teknik data mining untuk menemukan aturan assosiatif antara suatu kombinasi item. Contoh aturan assosiatif dari analisa pembelian di suatu pasar swalayan adalah dapat diketahuinya berapa besar kemungkinan seorang pelanggan membeli roti bersamaan dengan susu. Dengan pengetahuan tersebut pemilik pasar swalayan dapat mengatur penempatan barangnya ataumerancang kampanye pemasaran dengan memakai kupon diskon untuk kombinasi barang tertentu. Analisis asosiasi menjadi terkenal karena aplikasinya untuk menganalisa isi keranjang belanja di pasar swalayan. Analisis asosiasi juga sering disebut dengan istilah market basket analysis. Analisis asosiasi dikenal juga sebagai salah satu teknik data mining yang menjadi dasar dari berbagai teknik data mining lainnya. Khususnya salah satu tahap dari analisis asosiasi yang disebut analisis pola frequensi tinggi (frequent pattern mining) menarik perhatian banyak peneliti untuk menghasilkan algoritma yang efisien. Penting tidaknya suatu aturan assosiatif dapat diketahui dengan dua parameter, support (nilai penunjang) yaitu persentase kombinasi item tersebut dalam database dan confidence (nilai kepastian) yaitu kuatnya hubungan antar item dalam aturan assosiatif. referensi http://kabarbee.com/pengertian-data-mining/","title":"Asosiasi"},{"location":"DecisionTree/","text":"Decision Tree (Pohon Keputusan) Konsep Salah satu metode data mining yang umum digunakan adalah pohon keputusan. Metode pohon keputusan mengubah fakta yang sangat besar menjadi pohon keputusan yang merepresentasikan rule. Pohon keputusan adalah salah satu metode klasifikasi yang paling popular karena mudah untuk diinterpretasi oleh manusia. Konsep dari pohon keputusan adalah mengubah data menjadi pohon keputusan dan aturan-aturan keputusan. Data dalam pohon keputusan biasanya dinyatakan dalam bentuk tabel dengan atribut dan record. Atribut menyatakan suatu parameter yang dibuat sebagai kriteria dalam pembentukan tree Proses pada pohon keputusan adalah mengubah bentuk data (tabel) menjadi model pohon, mengubah model pohon menjadi rule, dan menyederhanakan rule. Manfaat utama dari penggunaan pohon keputusan adalah kemampuannya untuk membreak down proses pengambilan keputusan yang kompleks menjadi lebih simpel sehingga pengambil keputusan akan lebih menginterpretasikan solusi dari permasalahan. Pohon Keputusan juga berguna untuk mengeksplorasi data, menemukan hubungan tersembunyi antara sejumlah calon variabel input dengan sebuah variabel target. Pohon keputusan merupakan himpunan aturan IF\u2026THEN. Setiap path dalam tree dihubungkan dengan sebuah aturan, di mana premis terdiri atas sekumpulan node-node yang ditemui, dan kesimpulan dari aturam terdiri atas kelas yang terhubung dengan leaf dari path. Bagian awal dari pohon keputusan ini adalah titik akar (root), sedangkan setiap cabang dari pohon keputusan merupakan pembagian berdasarkan hasil uji, dan titik akhir (leaf) merupakan pembagian kelas yang dihasilkan. Pohon keputusan mempunyai 3 tipe simpul yaitu: 1. Simpul akar, dimana tidak memiliki cabang yang masuk dan memiliki cabang lebih dari satu, terkadang tidak memiliki cabang sama sekali. Simpul ini biasanya berupa atribut yang paling memiliki pengaruh terbesar pada suatu kelas tertentu. 2. Simpul internal, dimana hanya memiliki 1 cabang yang masuk, dan memiliki lebih dari 1 cabang yang keluar. 3. Simpul daun, atau simpul akhir dimana hanya memiliki 1 cabang yang masuk, dan tidak memiliki cabang sama sekali dan menandai bahwa simpul tersebut merupakan label kelas. Tahap awal dilakukan pengujian simpul akar, jika pada pengujian simpul akar menghasilkan sesuatu maka proses pengujian juga dilakukan pada setiap cabang berdasarkan hasil dari pengujian. Hal ini berlaku juga untuk simpul internal dimana suatu kondisi pengujian baru akan diterapkan pada simpul daun. Pada umumnya proses dari sistem pohon keputusan adalah mengadopsi strategi pencarian top-down untuk solusi ruang pencariannya. Pada proses mengklasifikasikan sampel yang tidak diketahui, nilai atribut akan diuji pada pohon keputusan dengan cara melacak jalur dari titik akar sampai titik akhir, kemudian akan diprediksikan kelas yang ditempati sampel baru tersebut. Pohon keputusan banyak digunakan dalam proses data mining karena memiliki beberapa kelebihan, yaitu: 1. Tidak memerlukan biaya yang mahal saat membangun algoritma. 2. Mudah untuk diinterpetasikan. 3. Mudah mengintegrasikan dengan sistem basis data. 4. Memiliki nilai ketelitian yang lebih baik. 5. Dapat menemukan hubungan tak terduga dan suatu data. 6. Dapat menggunakan data pasti/mutlak atau data kontinu. 7. Mengakomodasi data yang hilang. Algoritma Pohon Keputusan Salah satu algoritma induksi pohon keputusan yaitu ID3 (Iterative Dichotomiser 3). ID3 dikembangkan oleh J. Ross Quinlan. Dalam prosedur algoritma ID3, input berupa sampel training, label training dan atribut. Algoritma C4.5 merupakan pengembangan dari ID3. Sedangkan pada perangkat lunak open source WEKA mempunyai versi sendiri C4.5 yang dikenal sebagai J48. Pohon dibangun dengan cara membagi data secara rekursif hingga tiap bagian terdiri dari data yang berasal dari kelas yang sama. Bentuk pemecahan (split) yang digunakan untuk membagi data tergantung dari jenis atribut yang digunakan dalam split. Algoritma C4.5 dapat menangani data numerik (kontinyu) dan diskret. Split untuk atribut numerik yaitu mengurutkan contoh berdasarkan atribut kontiyu A, kemudian membentuk minimum permulaan (threshold) M dari contoh-contoh yang ada dari kelas mayoritas pada setiap partisi yang bersebelahan, lalu menggabungkan partisi-partisi yang bersebelahan tersebut dengan kelas mayoritas yang sama. Split untuk atribut diskret A mempunyai bentuk value (A) \u03b5 X dimana X \u2282 domain(A). Jika suatu set data mempunyai beberapa pengamatan dengan missing value yaitu record dengan beberapa nilai variabel tidak ada, Jika jumlah pengamatan terbatas maka atribut dengan missing value dapat diganti dengan nilai rata-rata dari variabel yang bersangkutan.[Santosa,2007] Untuk melakukan pemisahan obyek (split) dilakukan tes terhadap atribut dengan mengukur tingkat ketidakmurnian pada sebuah simpul (node). Pada algoritma C.45 menggunakan rasio perolehan (gain ratio). Sebelum menghitung rasio perolehan, perlu menghitung dulu nilai informasi dalam satuan bits dari suatu kumpulan objek. Cara menghitungnya dilakukan dengan menggunakan konsep entropi. S adalah ruang (data) sampel yang digunakan untuk pelatihan, p+ adalah jumlah yang bersolusi positif atau mendukung pada data sampel untuk kriteria tertentu dan p- adalah jumlah yang bersolusi negatif atau tidak mendukung pada data sampel untuk kriteria tertentu. ntropi(S) sama dengan 0, jika semua contoh pada S berada dalam kelas yang sama. Entropi(S) sama dengan 1, jika jumlah contoh positif dan negative dalam S adalah sama. Entropi(S) lebih dari 0 tetapi kurang dari 1, jika jumlah contoh positif dan negative dalam S tidak sama [Mitchell,1997].Entropi split yang membagi S dengan n record menjadi himpunan-himpunan S1 dengan n1 baris dan S2 dengan n2 baris adalah : Kemudian menghitung perolehan informasi dari output data atau variabel dependent y yang dikelompokkan berdasarkan atribut A, dinotasikan dengan gain (y,A). Perolehan informasi, gain (y,A), dari atribut A relative terhadap output data y adalah: nilai (A) adalah semua nilai yang mungkin dari atribut A, dan yc adalah subset dari y dimana A mempunyai nilai c. Term pertama dalam persamaan diatas adalah entropy total y dan term kedua adalah entropy sesudah dilakukan pemisahan data berdasarkan atribut A. Untuk menghitung rasio perolehan perlu diketahui suatu term baru yang disebut pemisahan informasi (SplitInfo). Pemisahan informasi dihitung dengan cara : bahwa S1 sampai Sc adalah c subset yang dihasilkan dari pemecahan S dengan menggunakan atribut A yang mempunyai sebanyak c nilai. Selanjutnya rasio perolehan (gain ratio) dihitung dengan cara : Implementasi studi kasus kali ini, saya akan menggunakan dataset user modeling. dataset ini memiliki 6 kolom dan memiliki 3 klasifikasi kelas. berikut merupakan data yang saya gunakan disini langsung ke programnya : import pandas as pd from sklearn.tree import DecisionTreeClassifier from sklearn import model_selection from sklearn import metrics from sklearn.model_selection import train_test_split from sklearn.tree import export_graphviz from sklearn.externals.six import StringIO import pydotplus from IPython.display import Image kemudian membaca file csv nya df=pd.read_csv( data.csv ) menampilkan datanya : print( Informasi Data\\n ) print( Jumlah Data : , len(df)) print ( Dimensi Data : ,df.shape) print ( Dataset : ) print(df.head()) print( \\n ) 10 baris pertama : df.head(10) splitting dataset ke training dan testing train, test = train_test_split(df, test_size = 0.1,random_state=1234) #mencari hasil print(train.shape) print(test.shape) # Dataset validasi dataset array = df.values X = array[:,1:5] Y = array[:,5] # Sepertiga data sebagai bagian dari set tes validation_size = 15 seed = 7 X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed) mencari Hasil : print(X_train.shape) print(Y_train.shape) print(X_validation.shape) print(Y_validation.shape) entropy= DecisionTreeClassifier(criterion= entropy ,random_state=1234) #learning entropy.fit(X_train,Y_train) #Prediksi prediction=entropy.predict(X_validation) #mengevaluasi(Accuracy) print( Accuracy: ,metrics.accuracy_score(prediction,Y_validation)) #evaluation(Confusion Metrix) print( Confusion Metrix:\\n ,metrics.confusion_matrix(prediction,Y_validation)) feature_cols=[ SCG , STR , LPR , PEG ] dot_data = StringIO() export_graphviz(entropy, out_file=dot_data, filled=True, rounded=True, special_characters=True,feature_names = feature_cols,class_names=[ very_low , high , low , middle ]) graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) graph.write_png( entropy.png ) Image(graph.create_png()) print( Hasil prediksi menngunakan entropy ) #Membentuk kembali diperlukan untuk melakukan penggabungan pred_clf_df = pd.DataFrame(prediction.reshape(15,1)) #Ganti nama kolom untuk menunjukkan prediksi pred_clf_df.rename(columns={0: Prediction }, inplace=True) #membentuk kembali dataset uji X_validation_df = pd.DataFrame(X_validation.reshape(15 ,4)) #menggabungkan dua bingkai data panda di atas kolom untuk membuat dataset prediksi pred_outcome = pd.concat([X_validation_df, pred_clf_df], axis=1, join_axes=[X_validation_df.index]) pred_outcome.rename(columns = {0: STG ,1: SCG ,2: STR ,3: LPR ,4: PEG }, inplace=True) #cetak 10 baris prediksi akhir print((pred_outcome).head(15)) print ( \\n ) #mengevaluasi(Accuracy) print( Accuracy: ,metrics.accuracy_score(prediction,Y_validation)) Referensi : https://medium.com/@mimubarok.mim/decision-tree-pohon-keputusan-6484ad30c289 https://sharingaddicted.com/pengambilan-keputusan-menggunakan-pohon-keputusan/","title":"Decision Tree"},{"location":"DecisionTree/#decision-tree-pohon-keputusan","text":"","title":"Decision Tree (Pohon Keputusan)"},{"location":"DecisionTree/#konsep","text":"Salah satu metode data mining yang umum digunakan adalah pohon keputusan. Metode pohon keputusan mengubah fakta yang sangat besar menjadi pohon keputusan yang merepresentasikan rule. Pohon keputusan adalah salah satu metode klasifikasi yang paling popular karena mudah untuk diinterpretasi oleh manusia. Konsep dari pohon keputusan adalah mengubah data menjadi pohon keputusan dan aturan-aturan keputusan. Data dalam pohon keputusan biasanya dinyatakan dalam bentuk tabel dengan atribut dan record. Atribut menyatakan suatu parameter yang dibuat sebagai kriteria dalam pembentukan tree Proses pada pohon keputusan adalah mengubah bentuk data (tabel) menjadi model pohon, mengubah model pohon menjadi rule, dan menyederhanakan rule. Manfaat utama dari penggunaan pohon keputusan adalah kemampuannya untuk membreak down proses pengambilan keputusan yang kompleks menjadi lebih simpel sehingga pengambil keputusan akan lebih menginterpretasikan solusi dari permasalahan. Pohon Keputusan juga berguna untuk mengeksplorasi data, menemukan hubungan tersembunyi antara sejumlah calon variabel input dengan sebuah variabel target. Pohon keputusan merupakan himpunan aturan IF\u2026THEN. Setiap path dalam tree dihubungkan dengan sebuah aturan, di mana premis terdiri atas sekumpulan node-node yang ditemui, dan kesimpulan dari aturam terdiri atas kelas yang terhubung dengan leaf dari path. Bagian awal dari pohon keputusan ini adalah titik akar (root), sedangkan setiap cabang dari pohon keputusan merupakan pembagian berdasarkan hasil uji, dan titik akhir (leaf) merupakan pembagian kelas yang dihasilkan. Pohon keputusan mempunyai 3 tipe simpul yaitu: 1. Simpul akar, dimana tidak memiliki cabang yang masuk dan memiliki cabang lebih dari satu, terkadang tidak memiliki cabang sama sekali. Simpul ini biasanya berupa atribut yang paling memiliki pengaruh terbesar pada suatu kelas tertentu. 2. Simpul internal, dimana hanya memiliki 1 cabang yang masuk, dan memiliki lebih dari 1 cabang yang keluar. 3. Simpul daun, atau simpul akhir dimana hanya memiliki 1 cabang yang masuk, dan tidak memiliki cabang sama sekali dan menandai bahwa simpul tersebut merupakan label kelas. Tahap awal dilakukan pengujian simpul akar, jika pada pengujian simpul akar menghasilkan sesuatu maka proses pengujian juga dilakukan pada setiap cabang berdasarkan hasil dari pengujian. Hal ini berlaku juga untuk simpul internal dimana suatu kondisi pengujian baru akan diterapkan pada simpul daun. Pada umumnya proses dari sistem pohon keputusan adalah mengadopsi strategi pencarian top-down untuk solusi ruang pencariannya. Pada proses mengklasifikasikan sampel yang tidak diketahui, nilai atribut akan diuji pada pohon keputusan dengan cara melacak jalur dari titik akar sampai titik akhir, kemudian akan diprediksikan kelas yang ditempati sampel baru tersebut. Pohon keputusan banyak digunakan dalam proses data mining karena memiliki beberapa kelebihan, yaitu: 1. Tidak memerlukan biaya yang mahal saat membangun algoritma. 2. Mudah untuk diinterpetasikan. 3. Mudah mengintegrasikan dengan sistem basis data. 4. Memiliki nilai ketelitian yang lebih baik. 5. Dapat menemukan hubungan tak terduga dan suatu data. 6. Dapat menggunakan data pasti/mutlak atau data kontinu. 7. Mengakomodasi data yang hilang.","title":"Konsep"},{"location":"DecisionTree/#algoritma-pohon-keputusan","text":"Salah satu algoritma induksi pohon keputusan yaitu ID3 (Iterative Dichotomiser 3). ID3 dikembangkan oleh J. Ross Quinlan. Dalam prosedur algoritma ID3, input berupa sampel training, label training dan atribut. Algoritma C4.5 merupakan pengembangan dari ID3. Sedangkan pada perangkat lunak open source WEKA mempunyai versi sendiri C4.5 yang dikenal sebagai J48. Pohon dibangun dengan cara membagi data secara rekursif hingga tiap bagian terdiri dari data yang berasal dari kelas yang sama. Bentuk pemecahan (split) yang digunakan untuk membagi data tergantung dari jenis atribut yang digunakan dalam split. Algoritma C4.5 dapat menangani data numerik (kontinyu) dan diskret. Split untuk atribut numerik yaitu mengurutkan contoh berdasarkan atribut kontiyu A, kemudian membentuk minimum permulaan (threshold) M dari contoh-contoh yang ada dari kelas mayoritas pada setiap partisi yang bersebelahan, lalu menggabungkan partisi-partisi yang bersebelahan tersebut dengan kelas mayoritas yang sama. Split untuk atribut diskret A mempunyai bentuk value (A) \u03b5 X dimana X \u2282 domain(A). Jika suatu set data mempunyai beberapa pengamatan dengan missing value yaitu record dengan beberapa nilai variabel tidak ada, Jika jumlah pengamatan terbatas maka atribut dengan missing value dapat diganti dengan nilai rata-rata dari variabel yang bersangkutan.[Santosa,2007] Untuk melakukan pemisahan obyek (split) dilakukan tes terhadap atribut dengan mengukur tingkat ketidakmurnian pada sebuah simpul (node). Pada algoritma C.45 menggunakan rasio perolehan (gain ratio). Sebelum menghitung rasio perolehan, perlu menghitung dulu nilai informasi dalam satuan bits dari suatu kumpulan objek. Cara menghitungnya dilakukan dengan menggunakan konsep entropi. S adalah ruang (data) sampel yang digunakan untuk pelatihan, p+ adalah jumlah yang bersolusi positif atau mendukung pada data sampel untuk kriteria tertentu dan p- adalah jumlah yang bersolusi negatif atau tidak mendukung pada data sampel untuk kriteria tertentu. ntropi(S) sama dengan 0, jika semua contoh pada S berada dalam kelas yang sama. Entropi(S) sama dengan 1, jika jumlah contoh positif dan negative dalam S adalah sama. Entropi(S) lebih dari 0 tetapi kurang dari 1, jika jumlah contoh positif dan negative dalam S tidak sama [Mitchell,1997].Entropi split yang membagi S dengan n record menjadi himpunan-himpunan S1 dengan n1 baris dan S2 dengan n2 baris adalah : Kemudian menghitung perolehan informasi dari output data atau variabel dependent y yang dikelompokkan berdasarkan atribut A, dinotasikan dengan gain (y,A). Perolehan informasi, gain (y,A), dari atribut A relative terhadap output data y adalah: nilai (A) adalah semua nilai yang mungkin dari atribut A, dan yc adalah subset dari y dimana A mempunyai nilai c. Term pertama dalam persamaan diatas adalah entropy total y dan term kedua adalah entropy sesudah dilakukan pemisahan data berdasarkan atribut A. Untuk menghitung rasio perolehan perlu diketahui suatu term baru yang disebut pemisahan informasi (SplitInfo). Pemisahan informasi dihitung dengan cara : bahwa S1 sampai Sc adalah c subset yang dihasilkan dari pemecahan S dengan menggunakan atribut A yang mempunyai sebanyak c nilai. Selanjutnya rasio perolehan (gain ratio) dihitung dengan cara :","title":"Algoritma Pohon Keputusan"},{"location":"DecisionTree/#implementasi","text":"studi kasus kali ini, saya akan menggunakan dataset user modeling. dataset ini memiliki 6 kolom dan memiliki 3 klasifikasi kelas. berikut merupakan data yang saya gunakan disini langsung ke programnya : import pandas as pd from sklearn.tree import DecisionTreeClassifier from sklearn import model_selection from sklearn import metrics from sklearn.model_selection import train_test_split from sklearn.tree import export_graphviz from sklearn.externals.six import StringIO import pydotplus from IPython.display import Image kemudian membaca file csv nya df=pd.read_csv( data.csv ) menampilkan datanya : print( Informasi Data\\n ) print( Jumlah Data : , len(df)) print ( Dimensi Data : ,df.shape) print ( Dataset : ) print(df.head()) print( \\n ) 10 baris pertama : df.head(10) splitting dataset ke training dan testing train, test = train_test_split(df, test_size = 0.1,random_state=1234) #mencari hasil print(train.shape) print(test.shape) # Dataset validasi dataset array = df.values X = array[:,1:5] Y = array[:,5] # Sepertiga data sebagai bagian dari set tes validation_size = 15 seed = 7 X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed) mencari Hasil : print(X_train.shape) print(Y_train.shape) print(X_validation.shape) print(Y_validation.shape) entropy= DecisionTreeClassifier(criterion= entropy ,random_state=1234) #learning entropy.fit(X_train,Y_train) #Prediksi prediction=entropy.predict(X_validation) #mengevaluasi(Accuracy) print( Accuracy: ,metrics.accuracy_score(prediction,Y_validation)) #evaluation(Confusion Metrix) print( Confusion Metrix:\\n ,metrics.confusion_matrix(prediction,Y_validation)) feature_cols=[ SCG , STR , LPR , PEG ] dot_data = StringIO() export_graphviz(entropy, out_file=dot_data, filled=True, rounded=True, special_characters=True,feature_names = feature_cols,class_names=[ very_low , high , low , middle ]) graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) graph.write_png( entropy.png ) Image(graph.create_png()) print( Hasil prediksi menngunakan entropy ) #Membentuk kembali diperlukan untuk melakukan penggabungan pred_clf_df = pd.DataFrame(prediction.reshape(15,1)) #Ganti nama kolom untuk menunjukkan prediksi pred_clf_df.rename(columns={0: Prediction }, inplace=True) #membentuk kembali dataset uji X_validation_df = pd.DataFrame(X_validation.reshape(15 ,4)) #menggabungkan dua bingkai data panda di atas kolom untuk membuat dataset prediksi pred_outcome = pd.concat([X_validation_df, pred_clf_df], axis=1, join_axes=[X_validation_df.index]) pred_outcome.rename(columns = {0: STG ,1: SCG ,2: STR ,3: LPR ,4: PEG }, inplace=True) #cetak 10 baris prediksi akhir print((pred_outcome).head(15)) print ( \\n ) #mengevaluasi(Accuracy) print( Accuracy: ,metrics.accuracy_score(prediction,Y_validation)) Referensi : https://medium.com/@mimubarok.mim/decision-tree-pohon-keputusan-6484ad30c289 https://sharingaddicted.com/pengambilan-keputusan-menggunakan-pohon-keputusan/","title":"Implementasi"},{"location":"K-means/","text":"K-means Clustering K-Means Clustering adalah suatu metode penganalisaan data atau metode Data Mining yang melakukan proses pemodelan tanpa supervisi (unsupervised) dan merupakan salah satu metode yang melakukan pengelompokan data dengan sistem partisi. Metode K-Means Clustering berusaha mengelompokkan data yang ada ke dalam beberapa kelompok, dimana data dalam satu kelompok mempunyai karakteristik yang sama satu sama lainnya dan mempunyai karakteristik yang berbeda dengan data yang ada di dalam kelompok yang lain. Dengan kata lain, metode K-Means Clustering bertujuan untuk meminimalisasikan objective function yang diset dalam proses clustering dengan cara meminimalkan variasi antar data yang ada di dalam suatu cluster dan memaksimalkan variasi dengan data yang ada di cluster lainnya. Data clustering menggunakan metode K-Means Clustering ini secara umum dilakukan dengan algoritma dasar sebagai berikut: Algoritma K-means Clustering Langkah 1 \u2013 Menentukan secara acak K titik data sebagai pusat cluster yang disebut centroid. Langkah 2 \u2013 Menandai masing masing x_i masuk ke ke cluster tertentu, dengan cara menghitung jarak x_i ke masing masing pusat cluster (centroid) dan memasukkan x_i anggota pusat cluster tertentu tersebut jika memiliki jarak terdekat. Langkah 3 \u2013 Menentukan pusat cluster baru dengan menghitungya rata rata dari anggota cluster Langkah 4 \u2013 Ulangai langkah 2 dan 3 sampai tidak tidak ada dari anggota setiap cluster berubah tempat kelompoknya ## Implementasi Studi Kasus Saya akan mencoba metode K-means clustering ini menggunakan data berikut : dari data diatas, saya mengambil 3 class sebagai centroid data yaitu pada baris ke 3, 8 dan 13. centroid bisa diambil secara random. kemudian cari tiap jarak data pada masing-masing baris dengan centroid yang sudah dipilih dengan rumus seperti berikut : pada percobaan saya, berikut merupakan nilai jarak pada masing-masing baris dengan centroid dan sekaligus tentukan cluster nya , yaitu dengan cara ambil nilai terendah pada nilai jarak masing-masing class. saya kasih nama cluster 1, 2 dan 3 kemudian menentukan pusat cluster baru (centroid) dengan cara menghitung rata-rata dari masing-masing anggota cluster. dari rata-rata tersebut dapat diketahui nilai centroid baru. dari rata-rata yang ditemukan diatas, dapat ditentukan nilai centroid barunya yaitu sebagai berikut: setelah menemukan nilai centroid baru, maka Ulangai langkah 2 dan 3 sampai tidak tidak ada dari anggota setiap cluster berubah tempat kelompoknya. menghitung jarak data dengan centroid baru. berikut hasil perhitungan pada iterasi kedua: dari iterasi 2 diatas, didapatkan penempatan classnya sama pada iterasi 1, maka perhitungan sudah selesai referensi https://informatikalogi.com/algoritma-k-means-clustering/ https://yudiagusta.wordpress.com/k-means/","title":"K- Mean"},{"location":"K-means/#k-means-clustering","text":"K-Means Clustering adalah suatu metode penganalisaan data atau metode Data Mining yang melakukan proses pemodelan tanpa supervisi (unsupervised) dan merupakan salah satu metode yang melakukan pengelompokan data dengan sistem partisi. Metode K-Means Clustering berusaha mengelompokkan data yang ada ke dalam beberapa kelompok, dimana data dalam satu kelompok mempunyai karakteristik yang sama satu sama lainnya dan mempunyai karakteristik yang berbeda dengan data yang ada di dalam kelompok yang lain. Dengan kata lain, metode K-Means Clustering bertujuan untuk meminimalisasikan objective function yang diset dalam proses clustering dengan cara meminimalkan variasi antar data yang ada di dalam suatu cluster dan memaksimalkan variasi dengan data yang ada di cluster lainnya. Data clustering menggunakan metode K-Means Clustering ini secara umum dilakukan dengan algoritma dasar sebagai berikut:","title":"K-means Clustering"},{"location":"K-means/#algoritma-k-means-clustering","text":"Langkah 1 \u2013 Menentukan secara acak K titik data sebagai pusat cluster yang disebut centroid. Langkah 2 \u2013 Menandai masing masing x_i masuk ke ke cluster tertentu, dengan cara menghitung jarak x_i ke masing masing pusat cluster (centroid) dan memasukkan x_i anggota pusat cluster tertentu tersebut jika memiliki jarak terdekat. Langkah 3 \u2013 Menentukan pusat cluster baru dengan menghitungya rata rata dari anggota cluster Langkah 4 \u2013 Ulangai langkah 2 dan 3 sampai tidak tidak ada dari anggota setiap cluster berubah tempat kelompoknya ## Implementasi Studi Kasus Saya akan mencoba metode K-means clustering ini menggunakan data berikut : dari data diatas, saya mengambil 3 class sebagai centroid data yaitu pada baris ke 3, 8 dan 13. centroid bisa diambil secara random. kemudian cari tiap jarak data pada masing-masing baris dengan centroid yang sudah dipilih dengan rumus seperti berikut : pada percobaan saya, berikut merupakan nilai jarak pada masing-masing baris dengan centroid dan sekaligus tentukan cluster nya , yaitu dengan cara ambil nilai terendah pada nilai jarak masing-masing class. saya kasih nama cluster 1, 2 dan 3 kemudian menentukan pusat cluster baru (centroid) dengan cara menghitung rata-rata dari masing-masing anggota cluster. dari rata-rata tersebut dapat diketahui nilai centroid baru. dari rata-rata yang ditemukan diatas, dapat ditentukan nilai centroid barunya yaitu sebagai berikut: setelah menemukan nilai centroid baru, maka Ulangai langkah 2 dan 3 sampai tidak tidak ada dari anggota setiap cluster berubah tempat kelompoknya. menghitung jarak data dengan centroid baru. berikut hasil perhitungan pada iterasi kedua: dari iterasi 2 diatas, didapatkan penempatan classnya sama pada iterasi 1, maka perhitungan sudah selesai referensi https://informatikalogi.com/algoritma-k-means-clustering/ https://yudiagusta.wordpress.com/k-means/","title":"Algoritma K-means Clustering"},{"location":"KNN/","text":"K-Nearest Neighbors Konsep K-nearest neighbors atau knn adalah algoritma yang berfungsi untuk melakukan klasifikasi suatu data berdasarkan data pembelajaran ( train data sets ), yang diambil dari k tetangga terdekatnya ( nearest neighbors ). Dengan k merupakan banyaknya tetangga terdekat. Cara Kerja Algoritma K-Nearest Neighbors (KNN) K-nearest neighbors melakukan klasifikasi dengan proyeksi data pembelajaran pada ruang berdimensi banyak. Ruang ini dibagi menjadi bagian-bagian yang merepresentasikan kriteria data pembelajaran. Setiap data pembelajaran direpresentasikan menjadi titik-titik c pada ruang dimensi banyak Klasifikasi Terdekat (Nearest Neighbor Classification) Data baru yang diklasifikasi selanjutnya diproyeksikan pada ruang dimensi banyak yang telah memuat titik-titik c data pembelajaran. Proses klasifikasi dilakukan dengan mencari titik c terdekat dari c-baru ( nearest neighbor ) . Teknik pencarian tetangga terdekat yang umum dilakukan dengan menggunakan formula jarak euclidean*.* Berikut beberapa formula yang digunakan dalam algoritma knn. Euclidean Distance Jarak Euclidean adalah formula untuk mencari jarak antara 2 titik dalam ruang dua dimensi. Hamming Distance Jarak Hamming adalah cara mencari jarak antar 2 titik yang dihitung dengan panjang vektor biner yang dibentuk oleh dua titik tersebut dalam block kode biner. Manhattan Distance Manhattan Distance atau Taxicab Geometri adalah formula untuk mencari jarak d antar 2 vektor p,q pada ruang dimensi n Minkowski Distance Minkowski distance adalah formula pengukuran antar 2 titik pada ruang vektor normal yang merupakan hibridisasi yang mengeneralisasi euclidean distance dan mahattan distance. Teknik pencarian tetangga terdekat disesuaikan dengan dimensi data, proyeksi, dan kemudahan implementasi oleh pengguna. Banyaknya k Tetangga Terdekat Untuk menggunakan algoritma k nearest neighbors, perlu ditentukan banyaknya k tetangga terdekat yang digunakan untuk melakukan klasifikasi data baru. Banyaknya k, sebaiknya merupakan angka ganjil, misalnya k = 1, 2, 3, dan seterusnya. Penentuan nilai k dipertimbangkan berdasarkan banyaknya data yang ada dan ukuran dimensi yang dibentuk oleh data. Semakin banyak data yang ada, angka k yang dipilih sebaiknya semakin rendah. Namun, semakin besar ukuran dimensi data, angka k yang dipilih sebaiknya semakin tinggi. Algoritma K-Nearest Neighbors Tentukan k bilangan bulat positif berdasarkan ketersediaan data pembelajaran. Pilih tetangga terdekat dari data baru sebanyak k. Tentukan klasifikasi paling umum pada langkah (ii), dengan menggunakan frekuensi terbanyak. Keluaran klasifikasi dari data sampel baru. Kelebihan dan Kekurangan dari Algoritma K-NN Kelebihan Sangat nonlinear kNN merupakan salah satu algoritma (model) pembelajaran mesin yang bersifat nonparametrik. Pembahasan mengenai model parametrik dan model nonparametrik bisa menjadi artikel sendiri, namun secara singkat, definisi model nonparametrik adalah model yang tidak mengasumsikan apa-apa mengenai distribusi instance di dalam dataset. Model nonparametrik biasanya lebih sulit diinterpretasikan, namun salah satu kelebihannya adalah garis keputusan kelas yang dihasilkan model tersebut bisa jadi sangat fleksibel dan nonlinear. Mudah dipahami dan diimplementasikan Dari paparan yang diberikan dan penjelasan cara menghitung jarak dalam artikel ini, cukup jelas bahwa algoritma kNN mudah dipahami dan juga mudah dimplementasikan. Untuk mengklasifikasi instance x menggunakan kNN, kita cukup mendefinisikan fungsi untuk menghitung jarak antar-instance, menghitung jarak x dengan semua instance lainnya berdasarkan fungsi tersebut, dan menentukan kelas x sebagai kelas yang paling banyak muncul dalam k instance terdekat. Kekurangan Perlu menunjukkan parameter K (jumlah tetangga terdekat) Tidak menangani nilai hilang (missing value) secara implisit Jika terdapat nilai hilang pada satu atau lebih variabel dari suatu instance, perhitungan jarak instance tersebut dengan instance lainnya menjadi tidak terdefinisi. Bagaimana coba, menghitung jarak dalam ruang 3-dimensi jika salah satu dimensi hilang? Karenanya, sebelum menerapkan kNN kerap dilakukan imputasi untuk mengisi nilai-nilai hilang yang ada pada dataset. Contoh teknik imputasi yang paling umum adalah mengisi nilai hilang pada suatu variabel dengan nilai rata-rata variabel tersebut (mean imputation). Sensitif terhadap data pencilan (outlier) Seperti yang telah dijelaskan Ali pada artikel sebelumnya, kNN bisa jadi sangat fleksibel jika k kecil. Fleksibilitas ini mengakibatkan kNN cenderung sensitif terhadap data pencilan, khususnya pencilan yang terletak di \u201ctengah-tengah\u201d kelas yang berbeda. Lebih jelasnya, perhatikan ilustrasi di bawah. Pada gambar kiri, seluruh instance bisa diklasifikasikan dengan benar ke dalam kelas biru dan jingga. Tetapi, ketika ditambahkan instance biru di antara instance jingga, beberapa instance jingga menjadi salah terklasifikasi.Perlu dipilih k yang tepat untuk mengurangi dampak data pencilan dalam kNN. Rentan terhadap dimensionalitas yang tinggi Berbagai permasalahan yang timbul dari tingginya dimensionalitas (baca: banyaknya variabel) menimpa sebagian besar algoritma pembelajaran mesin, dan kNN adalah salah satu algoritma yang paling rentan terhadap tingginya dimensionalitas. Hal ini karena semakin banyak dimensi, ruang yang bisa ditempati instance semakin besar, sehingga semakin besar pula kemungkinan bahwa nearest neighbour dari suatu instance sebetulnya sama sekali tidak \u201cnear\u201c. Implementasi Studi Kasus pada metode Knn kali ini menggunakan dataset iris import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) from sklearn.neighbors import KNeighborsClassifier from sklearn import model_selection from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score #memuat file csv df = pd.read_csv ( Iris.csv ) #Inisialisasi Gaussian Naive Bayes clf = KNeighborsClassifier ( n_neighbors = 3 ) # Dataset validasi dataset array = df.values X = array [ :,1:5 ] Y = array [ :,5 ] # Sepertiga data sebagai bagian dari set tes validation_size = 15 seed = 7 X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split ( X, Y, test_size = validation_size, random_state = seed ) # Opsi tes dan metrik evaluasi scoring = accuracy #Menyesuaikan set training clf.fit ( X_train, Y_train ) #Predicting untuk Set Tes pred_clf = clf.predict ( X_validation ) #Buat file prediksi dengan gabungan data asli dan prediksi #Membentuk kembali diperlukan untuk melakukan penggabungan pred_clf_df = pd.DataFrame ( pred_clf.reshape ( 15 ,1 )) #Ganti nama kolom untuk menunjukkan prediksi pred_clf_df.rename ( columns ={ 0 : Prediction } , inplace = True ) #membentuk kembali dataset uji X_validation_df = pd.DataFrame ( X_validation.reshape ( 15 ,4 )) #menggabungkan dua bingkai data panda di atas kolom untuk membuat dataset prediksi pred_outcome = pd.concat ([ X_validation_df, pred_clf_df ] , axis = 1 , join_axes =[ X_validation_df.index ]) pred_outcome.rename ( columns = { 0 : SepalLengthCm , 1 : SepalWidthCm , 2 : PetalLengthCm , 3 : PetalWidthCm } , inplace = True ) del df [ Id ] #menggabungkan prediksi dengan dataset asli pred_comp = pd.merge ( df,pred_outcome, on =[ SepalLengthCm , SepalWidthCm , PetalLengthCm , PetalWidthCm ]) #cetak 10 baris prediksi akhir print (( pred_comp ) .head ( 15 )) print ( \\n ) # make prediction sl = input ( Enter sepal length (cm): ) sw = input ( Enter sepal width (cm): ) tl = input ( Enter tepal length (cm): ) tw = input ( Enter tepal width (cm): ) dataClass = clf.predict ([[ sl,sw,tl,tw ]]) print ( \\n ) print ( Prediction: ) , dataClass print ( \\n ) hasil run program : referensi https://informatikalogi.com/algoritma-k-nn-k-nearest-neighbor/ https://id.wikipedia.org/wiki/KNN","title":"K-NN"},{"location":"KNN/#k-nearest-neighbors","text":"","title":"K-Nearest Neighbors"},{"location":"KNN/#konsep","text":"K-nearest neighbors atau knn adalah algoritma yang berfungsi untuk melakukan klasifikasi suatu data berdasarkan data pembelajaran ( train data sets ), yang diambil dari k tetangga terdekatnya ( nearest neighbors ). Dengan k merupakan banyaknya tetangga terdekat.","title":"Konsep"},{"location":"KNN/#cara-kerja-algoritma-k-nearest-neighbors-knn","text":"K-nearest neighbors melakukan klasifikasi dengan proyeksi data pembelajaran pada ruang berdimensi banyak. Ruang ini dibagi menjadi bagian-bagian yang merepresentasikan kriteria data pembelajaran. Setiap data pembelajaran direpresentasikan menjadi titik-titik c pada ruang dimensi banyak","title":"Cara Kerja Algoritma K-Nearest Neighbors (KNN)"},{"location":"KNN/#klasifikasi-terdekat-nearest-neighbor-classification","text":"Data baru yang diklasifikasi selanjutnya diproyeksikan pada ruang dimensi banyak yang telah memuat titik-titik c data pembelajaran. Proses klasifikasi dilakukan dengan mencari titik c terdekat dari c-baru ( nearest neighbor ) . Teknik pencarian tetangga terdekat yang umum dilakukan dengan menggunakan formula jarak euclidean*.* Berikut beberapa formula yang digunakan dalam algoritma knn.","title":"Klasifikasi Terdekat (Nearest Neighbor Classification)"},{"location":"KNN/#euclidean-distance","text":"Jarak Euclidean adalah formula untuk mencari jarak antara 2 titik dalam ruang dua dimensi.","title":"Euclidean Distance"},{"location":"KNN/#hamming-distance","text":"Jarak Hamming adalah cara mencari jarak antar 2 titik yang dihitung dengan panjang vektor biner yang dibentuk oleh dua titik tersebut dalam block kode biner.","title":"Hamming Distance"},{"location":"KNN/#manhattan-distance","text":"Manhattan Distance atau Taxicab Geometri adalah formula untuk mencari jarak d antar 2 vektor p,q pada ruang dimensi n","title":"Manhattan Distance"},{"location":"KNN/#minkowski-distance","text":"Minkowski distance adalah formula pengukuran antar 2 titik pada ruang vektor normal yang merupakan hibridisasi yang mengeneralisasi euclidean distance dan mahattan distance. Teknik pencarian tetangga terdekat disesuaikan dengan dimensi data, proyeksi, dan kemudahan implementasi oleh pengguna.","title":"Minkowski Distance"},{"location":"KNN/#banyaknya-k-tetangga-terdekat","text":"Untuk menggunakan algoritma k nearest neighbors, perlu ditentukan banyaknya k tetangga terdekat yang digunakan untuk melakukan klasifikasi data baru. Banyaknya k, sebaiknya merupakan angka ganjil, misalnya k = 1, 2, 3, dan seterusnya. Penentuan nilai k dipertimbangkan berdasarkan banyaknya data yang ada dan ukuran dimensi yang dibentuk oleh data. Semakin banyak data yang ada, angka k yang dipilih sebaiknya semakin rendah. Namun, semakin besar ukuran dimensi data, angka k yang dipilih sebaiknya semakin tinggi.","title":"Banyaknya k Tetangga Terdekat"},{"location":"KNN/#algoritma-k-nearest-neighbors","text":"Tentukan k bilangan bulat positif berdasarkan ketersediaan data pembelajaran. Pilih tetangga terdekat dari data baru sebanyak k. Tentukan klasifikasi paling umum pada langkah (ii), dengan menggunakan frekuensi terbanyak. Keluaran klasifikasi dari data sampel baru.","title":"Algoritma K-Nearest Neighbors"},{"location":"KNN/#kelebihan-dan-kekurangan-dari-algoritma-k-nn","text":"Kelebihan Sangat nonlinear kNN merupakan salah satu algoritma (model) pembelajaran mesin yang bersifat nonparametrik. Pembahasan mengenai model parametrik dan model nonparametrik bisa menjadi artikel sendiri, namun secara singkat, definisi model nonparametrik adalah model yang tidak mengasumsikan apa-apa mengenai distribusi instance di dalam dataset. Model nonparametrik biasanya lebih sulit diinterpretasikan, namun salah satu kelebihannya adalah garis keputusan kelas yang dihasilkan model tersebut bisa jadi sangat fleksibel dan nonlinear. Mudah dipahami dan diimplementasikan Dari paparan yang diberikan dan penjelasan cara menghitung jarak dalam artikel ini, cukup jelas bahwa algoritma kNN mudah dipahami dan juga mudah dimplementasikan. Untuk mengklasifikasi instance x menggunakan kNN, kita cukup mendefinisikan fungsi untuk menghitung jarak antar-instance, menghitung jarak x dengan semua instance lainnya berdasarkan fungsi tersebut, dan menentukan kelas x sebagai kelas yang paling banyak muncul dalam k instance terdekat. Kekurangan Perlu menunjukkan parameter K (jumlah tetangga terdekat) Tidak menangani nilai hilang (missing value) secara implisit Jika terdapat nilai hilang pada satu atau lebih variabel dari suatu instance, perhitungan jarak instance tersebut dengan instance lainnya menjadi tidak terdefinisi. Bagaimana coba, menghitung jarak dalam ruang 3-dimensi jika salah satu dimensi hilang? Karenanya, sebelum menerapkan kNN kerap dilakukan imputasi untuk mengisi nilai-nilai hilang yang ada pada dataset. Contoh teknik imputasi yang paling umum adalah mengisi nilai hilang pada suatu variabel dengan nilai rata-rata variabel tersebut (mean imputation). Sensitif terhadap data pencilan (outlier) Seperti yang telah dijelaskan Ali pada artikel sebelumnya, kNN bisa jadi sangat fleksibel jika k kecil. Fleksibilitas ini mengakibatkan kNN cenderung sensitif terhadap data pencilan, khususnya pencilan yang terletak di \u201ctengah-tengah\u201d kelas yang berbeda. Lebih jelasnya, perhatikan ilustrasi di bawah. Pada gambar kiri, seluruh instance bisa diklasifikasikan dengan benar ke dalam kelas biru dan jingga. Tetapi, ketika ditambahkan instance biru di antara instance jingga, beberapa instance jingga menjadi salah terklasifikasi.Perlu dipilih k yang tepat untuk mengurangi dampak data pencilan dalam kNN. Rentan terhadap dimensionalitas yang tinggi Berbagai permasalahan yang timbul dari tingginya dimensionalitas (baca: banyaknya variabel) menimpa sebagian besar algoritma pembelajaran mesin, dan kNN adalah salah satu algoritma yang paling rentan terhadap tingginya dimensionalitas. Hal ini karena semakin banyak dimensi, ruang yang bisa ditempati instance semakin besar, sehingga semakin besar pula kemungkinan bahwa nearest neighbour dari suatu instance sebetulnya sama sekali tidak \u201cnear\u201c.","title":"Kelebihan dan Kekurangan dari Algoritma K-NN"},{"location":"KNN/#implementasi-studi-kasus","text":"pada metode Knn kali ini menggunakan dataset iris import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) from sklearn.neighbors import KNeighborsClassifier from sklearn import model_selection from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score #memuat file csv df = pd.read_csv ( Iris.csv ) #Inisialisasi Gaussian Naive Bayes clf = KNeighborsClassifier ( n_neighbors = 3 ) # Dataset validasi dataset array = df.values X = array [ :,1:5 ] Y = array [ :,5 ] # Sepertiga data sebagai bagian dari set tes validation_size = 15 seed = 7 X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split ( X, Y, test_size = validation_size, random_state = seed ) # Opsi tes dan metrik evaluasi scoring = accuracy #Menyesuaikan set training clf.fit ( X_train, Y_train ) #Predicting untuk Set Tes pred_clf = clf.predict ( X_validation ) #Buat file prediksi dengan gabungan data asli dan prediksi #Membentuk kembali diperlukan untuk melakukan penggabungan pred_clf_df = pd.DataFrame ( pred_clf.reshape ( 15 ,1 )) #Ganti nama kolom untuk menunjukkan prediksi pred_clf_df.rename ( columns ={ 0 : Prediction } , inplace = True ) #membentuk kembali dataset uji X_validation_df = pd.DataFrame ( X_validation.reshape ( 15 ,4 )) #menggabungkan dua bingkai data panda di atas kolom untuk membuat dataset prediksi pred_outcome = pd.concat ([ X_validation_df, pred_clf_df ] , axis = 1 , join_axes =[ X_validation_df.index ]) pred_outcome.rename ( columns = { 0 : SepalLengthCm , 1 : SepalWidthCm , 2 : PetalLengthCm , 3 : PetalWidthCm } , inplace = True ) del df [ Id ] #menggabungkan prediksi dengan dataset asli pred_comp = pd.merge ( df,pred_outcome, on =[ SepalLengthCm , SepalWidthCm , PetalLengthCm , PetalWidthCm ]) #cetak 10 baris prediksi akhir print (( pred_comp ) .head ( 15 )) print ( \\n ) # make prediction sl = input ( Enter sepal length (cm): ) sw = input ( Enter sepal width (cm): ) tl = input ( Enter tepal length (cm): ) tw = input ( Enter tepal width (cm): ) dataClass = clf.predict ([[ sl,sw,tl,tw ]]) print ( \\n ) print ( Prediction: ) , dataClass print ( \\n ) hasil run program : referensi https://informatikalogi.com/algoritma-k-nn-k-nearest-neighbor/ https://id.wikipedia.org/wiki/KNN","title":"Implementasi Studi Kasus"}]}